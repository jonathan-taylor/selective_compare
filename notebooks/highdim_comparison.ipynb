{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import os\n",
    "import ipyparallel as ipp\n",
    "rc = ipp.Client()\n",
    "v = rc.load_balanced_view()\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "signal_opts={'signal':4.5, \n",
    "             'n':2000, \n",
    "             'p':2500, \n",
    "             'k':30,\n",
    "             'sigma':2}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n",
      "[stdout:1] \n",
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n",
      "[stdout:2] \n",
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n",
      "[stdout:3] \n",
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[0:2]: \u001b[0m\n",
       "array(['knockoff', 'selectiveInference', 'survival', 'intervals', 'glmnet',\n",
       "       'foreach', 'Matrix', 'tools', 'stats', 'graphics', 'grDevices',\n",
       "       'utils', 'datasets', 'methods', 'base'], \n",
       "      dtype='|S18')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[1:2]: \u001b[0m\n",
       "array(['knockoff', 'selectiveInference', 'survival', 'intervals', 'glmnet',\n",
       "       'foreach', 'Matrix', 'tools', 'stats', 'graphics', 'grDevices',\n",
       "       'utils', 'datasets', 'methods', 'base'], \n",
       "      dtype='|S18')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[2:2]: \u001b[0m\n",
       "array(['knockoff', 'selectiveInference', 'survival', 'intervals', 'glmnet',\n",
       "       'foreach', 'Matrix', 'tools', 'stats', 'graphics', 'grDevices',\n",
       "       'utils', 'datasets', 'methods', 'base'], \n",
       "      dtype='|S18')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[3:2]: \u001b[0m\n",
       "array(['knockoff', 'selectiveInference', 'survival', 'intervals', 'glmnet',\n",
       "       'foreach', 'Matrix', 'tools', 'stats', 'graphics', 'grDevices',\n",
       "       'utils', 'datasets', 'methods', 'base'], \n",
       "      dtype='|S18')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "\n",
    "signal_opts={'signal':4.5, \n",
    "             'n':2000, \n",
    "             'p':2500, \n",
    "             'k':30,\n",
    "             'sigma':2}\n",
    "\n",
    "from __future__ import division, print_function\n",
    "from IPython.display import HTML\n",
    "\n",
    "%load_ext rpy2.ipython\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# this code below is available at https://github.com/jonathan-taylor/selective-inference\n",
    "from selection.algorithms.lasso import lasso\n",
    "from selection.algorithms.sqrt_lasso import choose_lambda\n",
    "\n",
    "%R library(glmnet)\n",
    "%R library(selectiveInference)\n",
    "%R library(knockoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generating mechanism\n",
    "\n",
    "We use the same data generating mechanism as in [Barber and Candes (2016)](http://arxiv.org/abs/1602.03574)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.5836944802483181e-15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%px\n",
    "def cov(p, rho=0.25):\n",
    "    idx = np.arange(p)\n",
    "    return rho**np.fabs(np.subtract.outer(idx, idx))\n",
    "\n",
    "def sqrt_cov(p, rho=0.25):\n",
    "    idx = np.arange(p)\n",
    "    C = rho**np.fabs(np.subtract.outer(idx, idx))\n",
    "    return np.linalg.cholesky(C)\n",
    "\n",
    "# Testing we have the square-root correct\n",
    "p = 2500\n",
    "A = cov(p, rho=0.3)\n",
    "B = sqrt_cov(p, rho=0.3)\n",
    "np.linalg.norm(B.dot(B.T) - A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "cholesky_factors = {}\n",
    "for rho in [0, 0.25, 0.5, 0.75]:\n",
    "    cholesky_factors[(rho, 2500)] = sqrt_cov(2500, rho=rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "def instance(n=1000, k=30, p=400, signal=3.5, rho=0.25, sigma=1.234): # sigma there just to convince you \n",
    "                                                                       # we don't need to know noise level\n",
    "    if (rho, p) in cholesky_factors.keys():\n",
    "        _sqrt_cov = cholesky_factors[(rho, p)]\n",
    "    else:\n",
    "        cholesky_factors[(rho, p)] = sqrt_cov(p, rho=rho)\n",
    "        _sqrt_cov = cholesky_factors[(rho, p)]\n",
    "    X = np.random.standard_normal((n, p)).dot(_sqrt_cov.T)\n",
    "\n",
    "    X /= (np.sqrt((X**2).sum(0))) # like normc\n",
    "    beta = np.zeros(p)\n",
    "    beta[:k] = signal * (2 * np.random.binomial(1, 0.5, size=(k,)) - 1) \n",
    "    np.random.shuffle(beta)\n",
    "\n",
    "    Y = (X.dot(beta) + np.random.standard_normal(n)) * sigma\n",
    "    true_active = np.nonzero(beta != 0)[0]\n",
    "    return X, Y, true_active, beta\n",
    "\n",
    "X, Y, true_active, beta = instance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "def simulate(rho=np.arange(4)/4., q=0.2, kappa=[0.4,0.6,0.7,0.8,1.], do_cv=True, do_knockoff=True, signal_opts=signal_opts):\n",
    "    \n",
    "    n = signal_opts['n']\n",
    "    p = signal_opts['p']\n",
    "    sigma = signal_opts['sigma']\n",
    "    P0, PA, active_size = {}, {}, {}\n",
    "    full_model_FDP, full_model_power, directional_FDP = {}, {}, {}\n",
    "    effective_lam = {}\n",
    "    kappa = sorted(kappa)[::-1]\n",
    "\n",
    "    for r in rho:\n",
    "        signal_opts['rho'] = r\n",
    "        X, Y, true_active, beta = instance(**signal_opts)\n",
    "\n",
    "        theory_lam_lasso = np.mean(np.fabs(np.dot(X.T, np.random.standard_normal((n, 2000)))).max(0) * sigma)\n",
    "\n",
    "        # square-root LASSO at several kappa values\n",
    "\n",
    "        for _kap in kappa:\n",
    "\n",
    "            lam = choose_lambda(X)\n",
    "            L = lasso.sqrt_lasso(X, Y, _kap * lam)\n",
    "            L.fit()\n",
    "            S = L.summary('onesided')\n",
    "            \n",
    "            _kap = r'$\\kappa=%0.2f$' % _kap\n",
    "            active_size.setdefault((_kap, r), []).append(len(L.active))\n",
    "            effective_lam.setdefault((_kap, r), []).append(L._penalty.weights[0] / theory_lam_lasso)\n",
    "            \n",
    "            # keep p-values when screening \n",
    "        \n",
    "            if set(L.active).issuperset(true_active):\n",
    "                p0 = [_pv for _pv, v in zip(S['pval'], S['variable']) if v not in true_active]\n",
    "                pA = [_pv for _pv, v in zip(S['pval'], S['variable']) if v in true_active]\n",
    "            else:\n",
    "                p0 = []\n",
    "                pA = []\n",
    "                \n",
    "            P0.setdefault((_kap, r), []).extend(p0) \n",
    "            PA.setdefault((_kap, r), []).extend(pA)\n",
    "            if len(L.active) > 0:\n",
    "                selected = sm.stats.multipletests(S['pval'], q, 'fdr_bh')[0]\n",
    "            else:\n",
    "                selected = np.zeros(S.shape, np.bool)\n",
    "            type1_errors = selected * np.array([v not in true_active for v in S['variable']])\n",
    "            if hasattr(L, 'active_signs'):\n",
    "                sign_errors = selected * np.array([(s != np.sign(beta[v])) * (v in true_active) for v, s in \n",
    "                                                   zip(S['variable'], L.active_signs)])\n",
    "            else:\n",
    "                sign_errors = 0\n",
    "\n",
    "            full_model_FDP.setdefault((_kap, r), []).append(np.sum(type1_errors) / max(np.sum(selected), 1))\n",
    "            directional_FDP.setdefault((_kap, r), []).append(np.sum(type1_errors + sign_errors) / max(np.sum(selected), 1))\n",
    "            full_model_power.setdefault((_kap, r), []).append(np.sum(selected * np.array([v in true_active for v in S['variable']])) / len(true_active))\n",
    "                \n",
    "        # now do cv.glmnet and Lee et al. with\n",
    "        \n",
    "        if do_cv:\n",
    "            \n",
    "            %R -i X,Y Y = as.matrix(Y)\n",
    "            %R X = as.matrix(X)\n",
    "            %R G_CV = cv.glmnet(X, Y, standardize=FALSE, intercept=FALSE)\n",
    "            %R sigma_hat = estimateSigma(X, Y, standardize=FALSE, intercept=FALSE)$sigmahat\n",
    "            %R lamhat = G_CV$lambda.min\n",
    "            %R fit = glmnet(X, Y, standardize=FALSE,intercept=FALSE)\n",
    "            %R Yhat = predict(fit, X, s = lamhat)\n",
    "            %R nz = sum(predict(fit, s = lamhat, type = \"coef\") != 0)\n",
    "            %R sigma_hat = sqrt(sum((Y - Yhat)^2)/(length(Y) - nz - 1))\n",
    "\n",
    "            sigma_hatR = %R sigma_hat\n",
    "            lam_1SE = %R G_CV$lambda.1se\n",
    "            lam_minCV = %R G_CV$lambda.min\n",
    "    \n",
    "            lam_1SE *= n\n",
    "            lam_minCV *= n\n",
    "\n",
    "            for method, lam, sigma_hat in zip(['1SE', 'minCV', 'Lee et al.'], \n",
    "                                              [float(lam_1SE), float(lam_minCV), theory_lam_lasso],\n",
    "                                              [float(sigma_hatR), float(sigma_hatR), sigma]):\n",
    "                L = lasso.gaussian(X, Y, float(lam), sigma=sigma_hat)           \n",
    "                L.fit() \n",
    "                S = L.summary('onesided')\n",
    "\n",
    "                active_size.setdefault((method, r), []).append(len(L.active))\n",
    "                effective_lam.setdefault((method, r), []).append(L._penalty.weights[0] / theory_lam_lasso * sigma**2)\n",
    "\n",
    "                # keep p-values when screening \n",
    "        \n",
    "                if set(L.active).issuperset(true_active):\n",
    "                    p0 = [_pv for _pv, v in zip(S['pval'], S['variable']) if v not in true_active]\n",
    "                    pA = [_pv for _pv, v in zip(S['pval'], S['variable']) if v in true_active]\n",
    "                else:\n",
    "                    p0 = []\n",
    "                    pA = []\n",
    "\n",
    "                P0.setdefault((method, r), []).extend(p0) \n",
    "                PA.setdefault((method, r), []).extend(pA)\n",
    "\n",
    "                if len(L.active) > 0:\n",
    "                    selected = sm.stats.multipletests(S['pval'], q, 'fdr_bh')[0]\n",
    "                else:\n",
    "                    selected = np.zeros(S.shape, np.bool)\n",
    "\n",
    "                type1_errors = selected * np.array([v not in true_active for v in S['variable']])\n",
    "                if hasattr(L, 'active_signs'):\n",
    "                    sign_errors = selected * np.array([(s != np.sign(beta[v])) * (v in true_active) for v, s in \n",
    "                                                       zip(S['variable'], L.active_signs)])\n",
    "                else:\n",
    "                    sign_errors = 0\n",
    "\n",
    "                full_model_FDP.setdefault((method, r), []).append(np.sum(type1_errors) / max(np.sum(selected), 1))\n",
    "                directional_FDP.setdefault((method, r), []).append(np.sum(type1_errors + sign_errors) / max(np.sum(selected), 1))\n",
    "                full_model_power.setdefault((method, r), []).append(np.sum(selected * np.array([v in true_active for v in S['variable']])) / len(true_active))\n",
    "\n",
    "        if (signal_opts['n'] > 2*signal_opts['p']) and do_knockoff:\n",
    "\n",
    "            method = 'knockoff'\n",
    "            %R -i q\n",
    "            %R KO = knockoff.filter(X = X, y = y, fdr=q, threshold=\"knockoff\")$selected\n",
    "\n",
    "            selected_idx = %R KO\n",
    "            selected = np.zeros(p, np.bool)\n",
    "            if selected_idx is not None:\n",
    "                selected_idx = np.array(selected_idx) - 1 # R vs. python indexing\n",
    "                selected[selected_idx] = True\n",
    "                            \n",
    "            type1_errors = selected * np.array([v not in true_active for v in np.arange(p)])\n",
    "            sign_errors = 0 # don't know how to extract signs from R package\n",
    "\n",
    "            full_model_FDP.setdefault((method, r), []).append(np.sum(type1_errors) / max(np.sum(selected), 1))\n",
    "            directional_FDP.setdefault((method, r), []).append(np.sum(type1_errors + sign_errors) / max(np.sum(selected), 1))\n",
    "            full_model_power.setdefault((method, r), []).append(np.sum(selected * np.array([v in true_active for v in np.arange(p)])) / len(true_active))\n",
    "            \n",
    "    return P0, PA, full_model_FDP, directional_FDP, full_model_power, active_size, effective_lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@v.parallel(block=False)\n",
    "def batch(nullarg, **signal_opts):\n",
    "    return simulate(signal_opts=signal_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_data(outfilename, P0, PA, full_model_FDP, directional_FDP, full_model_power, active_size, effective_lam):\n",
    "    \n",
    "    if os.path.exists(outfilename):\n",
    "        old_results = np.load(outfilename)\n",
    "    else:\n",
    "        old_results = None\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    for v, n in zip([P0, PA, full_model_FDP, directional_FDP, full_model_power, active_size, effective_lam],\n",
    "                    ['P0', 'PA', 'full_model_FDP', 'directional_FDP', 'full_model_power', 'active_size', 'effective_lam']):\n",
    "        for key in full_model_FDP.keys():\n",
    "            nkey = \"__\".join([n] + [str(k) for k in key])\n",
    "            if old_results == None:\n",
    "                results[nkey] = v[key]\n",
    "            elif nkey in old_results:\n",
    "                results[nkey] = np.hstack([old_results[nkey], v[key]])\n",
    "    np.savez(outfilename, **results)\n",
    "    \n",
    "def load_data(infilename):\n",
    "    \n",
    "    results = np.load(infilename)\n",
    "\n",
    "    P0, PA, full_model_FDP, directional_FDP, full_model_power, active_size, effective_lam = {}, {}, {}, {}, {}, {}, {}\n",
    "    for v, n in zip([P0, PA, full_model_FDP, directional_FDP, full_model_power, active_size, effective_lam],\n",
    "                    ['P0', 'PA', 'full_model_FDP', 'directional_FDP', 'full_model_power', 'active_size', 'effective_lam']):\n",
    "        keys = [(key, key.split('__')[1:]) for key in results.keys() if key[:len(n)] == n]\n",
    "        for k1, k2 in keys:\n",
    "            _k, r = k2\n",
    "            r = float(r)\n",
    "            v[(_k,r)] = results[k1]\n",
    "\n",
    "    return P0, PA, full_model_FDP, directional_FDP, full_model_power, active_size, effective_lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def figure_panel(filename):\n",
    "    \n",
    "    P0, PA, full_model_FDP, directional_FDP, full_model_power, active_size, effective_lam = load_data(filename)\n",
    "\n",
    "    first_fig = plt.figure(figsize=(20,7))\n",
    "\n",
    "    # Model FDR\n",
    "\n",
    "    modelFDP_ax = plt.subplot(1,3,1)\n",
    "    modelFDP_dict = {}\n",
    "    for m, r in sorted(full_model_FDP.keys()):\n",
    "        modelFDP_dict.setdefault(m, []).append((r, np.mean(full_model_FDP[(m, r)])))\n",
    "    for m in sorted(modelFDP_dict.keys()):\n",
    "        modelFDP_dict[m] = np.array(modelFDP_dict[m])\n",
    "        modelFDP_ax.plot(modelFDP_dict[m][:,0], modelFDP_dict[m][:,1], label=m)\n",
    "    modelFDP_ax.legend(loc='lower left')\n",
    "    modelFDP_ax.set_ylabel(r'E(Model FDP)($\\rho$)', fontsize=20)\n",
    "    modelFDP_ax.set_xlabel(r'$\\rho$', fontsize=20)\n",
    "\n",
    "    # Directional FDR\n",
    "\n",
    "    dirFDP_ax =  plt.subplot(1,3,2)\n",
    "    dirFDP_dict = {}\n",
    "    for m, r in sorted(full_model_FDP.keys()):\n",
    "        dirFDP_dict.setdefault(m, []).append((r, np.mean(directional_FDP[(m, r)])))\n",
    "    for m in sorted(modelFDP_dict.keys()):\n",
    "        dirFDP_dict[m] = np.array(modelFDP_dict[m])\n",
    "        dirFDP_ax.plot(dirFDP_dict[m][:,0], dirFDP_dict[m][:,1], label=m)\n",
    "    dirFDP_ax.legend(loc='lower left')\n",
    "    dirFDP_ax.set_ylabel(r'E(Directional FDP)($\\rho$)', fontsize=20)\n",
    "    dirFDP_ax.set_xlabel(r'$\\rho$', fontsize=20)\n",
    "\n",
    "    # Power \n",
    "\n",
    "    power_ax = plt.subplot(1,3,3)\n",
    "    power_dict = {}\n",
    "    for m, r in sorted(full_model_FDP.keys()):\n",
    "        power_dict.setdefault(m, []).append((r, np.mean(full_model_power[(m, r)])))\n",
    "    for m in sorted(power_dict.keys()):\n",
    "        power_dict[m] = np.array(power_dict[m])\n",
    "        power_ax.plot(power_dict[m][:,0], power_dict[m][:,1], label=m)\n",
    "    power_ax.legend(loc='lower left')\n",
    "    power_ax.set_ylabel(r'Power($\\rho$)', fontsize=20)\n",
    "    power_ax.set_xlabel(r'$\\rho$', fontsize=20)\n",
    "    \n",
    "    first_fig.savefig(filename.split('.')[0] + '_figure_panel.pdf')\n",
    "    plt.close(first_fig)\n",
    "    \n",
    "def figure_null(filename):\n",
    "    \n",
    "    P0, PA, full_model_FDP, directional_FDP, full_model_power, active_size, effective_lam = load_data(filename)\n",
    "\n",
    "    null_fig = plt.figure(figsize=(12,12))\n",
    "    U = np.linspace(0, 1, 201)\n",
    "    null_ax =  null_fig.gca()\n",
    "    null_dict = {}\n",
    "    for m, r in sorted(full_model_FDP.keys()):\n",
    "        null_dict.setdefault(m, []).extend(P0[(m,r)])\n",
    "    for m in sorted(null_dict.keys()):\n",
    "        if null_dict[m]:\n",
    "            null_ax.plot(U, sm.distributions.ECDF(null_dict[m])(U), label=m)\n",
    "    null_ax.legend(loc='lower right')\n",
    "    null_ax.set_ylabel(r'ECDF(p)', fontsize=20)\n",
    "    null_ax.set_xlabel(r'$p', fontsize=20)\n",
    "\n",
    "    null_fig.savefig(filename.split('.')[0] + '_figure_null.pdf')\n",
    "    plt.close(null_fig)\n",
    "    \n",
    "def figure_alt(filename):\n",
    "    \n",
    "    P0, PA, full_model_FDP, directional_FDP, full_model_power, active_size, effective_lam = load_data(filename)\n",
    "\n",
    "    alt_fig = plt.figure(figsize=(12,12))\n",
    "    U = np.linspace(0, 1, 201)\n",
    "    alt_ax = alt_fig.gca()\n",
    "    alt_dict = {}\n",
    "    for m, r in sorted(full_model_FDP.keys()):\n",
    "        alt_dict.setdefault(m, []).extend(PA[(m,r)])\n",
    "    for m in sorted(alt_dict.keys()):\n",
    "        if alt_dict[m]:\n",
    "            alt_ax.plot(U, sm.distributions.ECDF(alt_dict[m])(U), label=m)\n",
    "    alt_ax.legend(loc='lower right')\n",
    "    alt_ax.set_ylabel(r'ECDF(p)', fontsize=20)\n",
    "    alt_ax.set_xlabel(r'$p', fontsize=20)\n",
    "\n",
    "    alt_fig.savefig(filename.split('.')[0] + '_figure_alt.pdf')\n",
    "    plt.close(alt_fig)\n",
    "\n",
    "def figure_active_set(filename):\n",
    "    \n",
    "    P0, PA, full_model_FDP, directional_FDP, full_model_power, active_size, effective_lam = load_data(filename)\n",
    "\n",
    "    active_fig = plt.figure(figsize=(12,12))\n",
    "    active_ax = active_fig.gca()\n",
    "    active_dict = {}\n",
    "    for m, r in sorted(full_model_FDP.keys()):\n",
    "        active_dict.setdefault(m, []).append((r, np.mean(active_size[(m, r)])))\n",
    "    for m in sorted(active_dict.keys()):\n",
    "        active_dict[m] = np.array(active_dict[m])\n",
    "        active_ax.plot(active_dict[m][:,0], active_dict[m][:,1], label=m)\n",
    "    active_ax.legend(loc='lower left')\n",
    "    active_ax.set_ylabel(r'E(# active)($\\rho$)', fontsize=20)\n",
    "    active_ax.set_xlabel(r'$\\rho$', fontsize=20)\n",
    "\n",
    "    active_fig.savefig(filename.split('.')[0] + '_figure_active.pdf')\n",
    "    plt.close(active_fig)\n",
    "    \n",
    "def figure_lam(filename):\n",
    "    \n",
    "    P0, PA, full_model_FDP, directional_FDP, full_model_power, active_size, effective_lam = load_data(filename)\n",
    "\n",
    "    lam_fig = plt.figure(figsize=(12,12))\n",
    "    lam_ax = lam_fig.gca()\n",
    "    lam_dict = {}\n",
    "    for m, r in sorted(full_model_FDP.keys()):\n",
    "        lam_dict.setdefault(m, []).append((r, np.mean(effective_lam[(m, r)])))\n",
    "    for m in sorted(lam_dict.keys()):\n",
    "        lam_dict[m] = np.array(lam_dict[m])\n",
    "        lam_ax.plot(lam_dict[m][:,0], lam_dict[m][:,1], label=m)\n",
    "    lam_ax.legend(loc='lower left')\n",
    "    lam_ax.set_ylabel(r'E($\\hat{\\lambda} / \\lambda_{theory}$)($\\rho$)', fontsize=20)\n",
    "    lam_ax.set_xlabel(r'$\\rho$', fontsize=20)\n",
    "\n",
    "    lam_fig.savefig(filename.split('.')[0] + '_figure_lam.pdf')\n",
    "    plt.close(lam_fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_plots(nsim=100, signal_opts=signal_opts):\n",
    "    \n",
    "    @v.parallel(block=False)\n",
    "    def batch(nullarg):\n",
    "        return simulate(signal_opts=signal_opts)\n",
    "    results = batch.map(range(nsim))\n",
    "\n",
    "    P0, PA, full_model_FDP, directional_FDP, full_model_power, active_size, effective_lam = {}, {}, {}, {}, {}, {}, {}\n",
    "    for i, r in enumerate(results):\n",
    "        _P0, _PA, _full_model_FDP, _directional_FDP, _full_model_power, _active_size, _effective_lam = r\n",
    "        for d1, d2 in zip([P0, PA, full_model_FDP, directional_FDP, full_model_power, active_size, effective_lam],\n",
    "                         [_P0, _PA, _full_model_FDP, _directional_FDP, _full_model_power, _active_size, _effective_lam]):\n",
    "            for key in d2.keys():\n",
    "                d1.setdefault(key,[]).extend(d2[key])\n",
    "        save_data('highdim.npz', P0, PA, full_model_FDP, directional_FDP, full_model_power, active_size, effective_lam)\n",
    "        print('result %d received' % i)\n",
    "        if True:\n",
    "        \n",
    "            figure_panel('highdim.npz')\n",
    "            figure_null('highdim.npz')\n",
    "            figure_alt('highdim.npz')\n",
    "            figure_lam('highdim.npz')\n",
    "            figure_active_set('highdim.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result 0 received\n",
      "result 1 received\n",
      "result 2 received\n",
      "result 3 received\n",
      "result 4 received\n",
      "result 5 received\n",
      "result 6 received\n",
      "result 7 received\n",
      "result 8 received\n",
      "result 9 received\n",
      "result 10 received\n",
      "result 11 received\n",
      "result 12 received\n",
      "result 13 received\n",
      "result 14 received\n",
      "result 15 received\n",
      "result 16 received\n",
      "result 17 received\n",
      "result 18 received\n",
      "result 19 received\n",
      "result 20 received\n",
      "result 21 received\n",
      "result 22 received\n",
      "result 23 received\n",
      "result 24 received\n",
      "result 25 received\n",
      "result 26 received\n",
      "result 27 received\n",
      "result 28 received\n",
      "result 29 received\n",
      "result 30 received\n",
      "result 31 received\n",
      "result 32 received\n",
      "result 33 received\n",
      "result 34 received\n",
      "result 35 received\n",
      "result 36 received\n",
      "result 37 received\n",
      "result 38 received\n",
      "result 39 received\n",
      "result 40 received\n",
      "result 41 received\n",
      "result 42 received\n",
      "result 43 received\n",
      "result 44 received\n",
      "result 45 received\n",
      "result 46 received\n",
      "result 47 received\n",
      "result 48 received\n",
      "result 49 received\n",
      "result 50 received\n",
      "result 51 received\n",
      "result 52 received\n",
      "result 53 received\n",
      "result 54 received\n",
      "result 55 received\n",
      "result 56 received\n",
      "result 57 received\n",
      "result 58 received\n",
      "result 59 received\n",
      "result 60 received\n",
      "result 61 received\n",
      "result 62 received\n",
      "result 63 received\n",
      "result 64 received\n",
      "result 65 received\n",
      "result 66 received\n",
      "result 67 received\n",
      "result 68 received\n",
      "result 69 received\n",
      "result 70 received\n",
      "result 71 received\n",
      "result 72 received\n",
      "result 73 received\n",
      "result 74 received\n",
      "result 75 received\n",
      "result 76 received\n",
      "result 77 received\n",
      "result 78 received\n",
      "result 79 received\n",
      "result 80 received\n",
      "result 81 received\n",
      "result 82 received\n",
      "result 83 received\n",
      "result 84 received\n",
      "result 85 received\n",
      "result 86 received\n",
      "result 87 received\n",
      "result 88 received\n",
      "result 89 received\n",
      "result 90 received\n",
      "result 91 received\n",
      "result 92 received\n",
      "result 93 received\n",
      "result 94 received\n",
      "result 95 received\n",
      "result 96 received\n",
      "result 97 received\n",
      "result 98 received\n",
      "result 99 received\n"
     ]
    }
   ],
   "source": [
    "make_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
